{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153719\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128222</th>\n",
       "      <td>여성혐오란  남성과 여성을 동등한 주체로 보지않을떄  어쨰 남자도 결혼 잘해야 인생...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88772</th>\n",
       "      <td>\"중국의 역사는 몽골 원나라에 지배당한 이후로 끝났잖아? ㅋㅋ\"</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32665</th>\n",
       "      <td>초짜는 무슨 초짜 그동안 안걸렸을뿐 초짜염색하다 왜했을까 ㅋ 캭퉤퉤</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98783</th>\n",
       "      <td>기자가 너무 엮네. 그럼 음주뺑소니 포함 음주운전 2번한 김흥국은?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146856</th>\n",
       "      <td>죄송합니다 문정부는 할줄아는게 없어요 ㅜㅜ어떻게 하면되죠? 라고 묻지도 않아요 그냥...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80893</th>\n",
       "      <td>\"중 국 놈 들 ..다 죽 어 버 려 라 !! 지구상에서 제일 쓰레기 인간들...\"</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100411</th>\n",
       "      <td>\"불교 기독교 없애라...결국 종교는 돈벌이 수단이다\"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153132</th>\n",
       "      <td>하다하다. 자율주행차도 젠드? 성인지감수성 있는 자율주행차가 있어야 겠다. 김여사 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113468</th>\n",
       "      <td>품위있는 그녀다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152826</th>\n",
       "      <td>2:15 아직도 지능 떨어지는 몇몇들은 요가의 나라니 여자 혼자다녀도 괜찮다느니 위...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  class\n",
       "128222  여성혐오란  남성과 여성을 동등한 주체로 보지않을떄  어쨰 남자도 결혼 잘해야 인생...      0\n",
       "88772                 \"중국의 역사는 몽골 원나라에 지배당한 이후로 끝났잖아? ㅋㅋ\"      7\n",
       "32665               초짜는 무슨 초짜 그동안 안걸렸을뿐 초짜염색하다 왜했을까 ㅋ 캭퉤퉤      0\n",
       "98783               기자가 너무 엮네. 그럼 음주뺑소니 포함 음주운전 2번한 김흥국은?      0\n",
       "146856  죄송합니다 문정부는 할줄아는게 없어요 ㅜㅜ어떻게 하면되죠? 라고 묻지도 않아요 그냥...      5\n",
       "80893      \"중 국 놈 들 ..다 죽 어 버 려 라 !! 지구상에서 제일 쓰레기 인간들...\"      7\n",
       "100411                     \"불교 기독교 없애라...결국 종교는 돈벌이 수단이다\"      3\n",
       "153132  하다하다. 자율주행차도 젠드? 성인지감수성 있는 자율주행차가 있어야 겠다. 김여사 ...      1\n",
       "113468                                           품위있는 그녀다      0\n",
       "152826  2:15 아직도 지능 떨어지는 몇몇들은 요가의 나라니 여자 혼자다녀도 괜찮다느니 위...      2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('../2_Preprocess/filtered_data.xlsx')\n",
    "print(len(df))\n",
    "df.sample(10)\n",
    "# !pip install JPype1-1.1.2-cp36-cp36m-win_amd64.whl\n",
    "# !pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = []\n",
    "\n",
    "for idx in range(len(df)):\n",
    "    tmp_text = df.loc[idx].text.replace(\"\\n\",\" \")\n",
    "    text_list.append(tmp_text)\n",
    "\n",
    "text_to_save = '\\n'.join(text_list)\n",
    "\n",
    "with open('before_tokenized.txt', 'w',encoding='utf-8') as file:\n",
    "    file.write(text_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, re, argparse\n",
    "# from khaiii import KhaiiiApi\n",
    "from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma\n",
    "\n",
    "\n",
    "def get_tokenizer(tokenizer_name):\n",
    "    if tokenizer_name == \"komoran\":\n",
    "        tokenizer = Komoran()\n",
    "    elif tokenizer_name == \"okt\":\n",
    "        tokenizer = Okt()\n",
    "    elif tokenizer_name == \"mecab\":\n",
    "        tokenizer = Mecab()\n",
    "    elif tokenizer_name == \"hannanum\":\n",
    "        tokenizer = Hannanum()\n",
    "    elif tokenizer_name == \"kkma\":\n",
    "        tokenizer = Kkma()\n",
    "    elif tokenizer_name == \"khaiii\":\n",
    "        tokenizer = KhaiiiApi()\n",
    "    else:\n",
    "        tokenizer = Mecab()\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def tokenize(tokenizer_name, corpus_fname, output_fname, pos=False):\n",
    "    tokenizer = get_tokenizer(tokenizer_name)\n",
    "    with open(corpus_fname, 'r', encoding='utf-8') as f1, \\\n",
    "            open(output_fname, 'w', encoding='utf-8') as f2:\n",
    "        for line in f1:\n",
    "            sentence = line.replace('\\n', '').strip()\n",
    "            if tokenizer_name == \"khaiii\":\n",
    "                tokens = []\n",
    "                for word in tokenizer.analyze(sentence):\n",
    "                    if pos:\n",
    "                        tokens.extend([str(m) for m in word.morphs])\n",
    "                    else:\n",
    "                        tokens.extend([str(m).split(\"/\")[0] for m in word.morphs])\n",
    "            else:\n",
    "                if pos:\n",
    "                    tokens = tokenizer.pos(sentence)\n",
    "                    tokens = [morph + \"/\" + tag for morph, tag in tokens]\n",
    "                else:\n",
    "                    tokens = tokenizer.morphs(sentence)\n",
    "            tokenized_sent = ' '.join(post_processing(tokens))\n",
    "            f2.writelines(tokenized_sent + '\\n')\n",
    "\n",
    "\n",
    "def post_processing(tokens):\n",
    "    results = []\n",
    "    for token in tokens:\n",
    "        # 숫자에 공백을 주어서 띄우기\n",
    "        processed_token = [el for el in re.sub(r\"(\\d)\", r\" \\1 \", token).split(\" \") if len(el) > 0]\n",
    "        results.extend(processed_token)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize('okt','./before_tokenized.txt','./okt_tokenized_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153719"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_listed = []\n",
    "with open('./okt_tokenized_text.txt', 'r',encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        tmp_list = line.split(' ')\n",
    "        tmp_list[-1] = tmp_list[-1].rstrip('\\n')\n",
    "        after_listed.append(tmp_list)\n",
    "len(after_listed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./list_sentences.pkl\",\"wb\") as f:\n",
    "    pickle.dump(after_listed, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
