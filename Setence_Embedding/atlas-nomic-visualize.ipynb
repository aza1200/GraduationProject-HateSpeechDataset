{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install emoji\n!pip install soynlp\n!pip install --upgrade nomic","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-16T13:32:30.024185Z","iopub.execute_input":"2024-01-16T13:32:30.024502Z","iopub.status.idle":"2024-01-16T13:33:09.627386Z","shell.execute_reply.started":"2024-01-16T13:32:30.024475Z","shell.execute_reply":"2024-01-16T13:33:09.626385Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: emoji in /opt/conda/lib/python3.10/site-packages (2.9.0)\nCollecting soynlp\n  Downloading soynlp-0.0.493-py3-none-any.whl (416 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.8/416.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.12.1 in /opt/conda/lib/python3.10/site-packages (from soynlp) (1.24.3)\nRequirement already satisfied: psutil>=5.0.1 in /opt/conda/lib/python3.10/site-packages (from soynlp) (5.9.3)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from soynlp) (1.11.4)\nRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from soynlp) (1.2.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->soynlp) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->soynlp) (3.2.0)\nInstalling collected packages: soynlp\nSuccessfully installed soynlp-0.0.493\nCollecting nomic\n  Downloading nomic-3.0.3.tar.gz (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nomic) (8.1.7)\nCollecting jsonlines (from nomic)\n  Obtaining dependency information for jsonlines from https://files.pythonhosted.org/packages/f8/62/d9ba6323b9202dd2fe166beab8a86d29465c41a0288cbe229fac60c1ab8d/jsonlines-4.0.0-py3-none-any.whl.metadata\n  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: loguru in /opt/conda/lib/python3.10/site-packages (from nomic) (0.7.2)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from nomic) (13.5.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from nomic) (2.31.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from nomic) (1.24.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from nomic) (2.0.3)\nRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from nomic) (1.10.12)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nomic) (4.66.1)\nRequirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (from nomic) (11.0.0)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from nomic) (9.5.0)\nRequirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonlines->nomic) (23.1.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->nomic) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->nomic) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->nomic) (2023.3)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->nomic) (4.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->nomic) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->nomic) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->nomic) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->nomic) (2023.11.17)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->nomic) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->nomic) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->nomic) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->nomic) (1.16.0)\nDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\nBuilding wheels for collected packages: nomic\n  Building wheel for nomic (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for nomic: filename=nomic-3.0.3-py3-none-any.whl size=40999 sha256=1921969c9432a0ad6dcc0fc2c257839bca7917e3819a7d614910b83def567580\n  Stored in directory: /root/.cache/pip/wheels/75/dc/63/d22dea87e1b53bd0ca3afba90432966c9ace2f773e8dcb5d5f\nSuccessfully built nomic\nInstalling collected packages: jsonlines, nomic\nSuccessfully installed jsonlines-4.0.0 nomic-3.0.3\n","output_type":"stream"}]},{"cell_type":"code","source":"import nomic\nimport getpass\n\n# API KEY Issue link : https://atlas.nomic.ai/cli-login\nYOUR_API_KEY =  getpass.getpass(\"Enter your Nomic API Key: \")\nnomic.login(YOUR_API_KEY)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T13:33:09.629248Z","iopub.execute_input":"2024-01-16T13:33:09.629565Z","iopub.status.idle":"2024-01-16T13:34:11.580593Z","shell.execute_reply.started":"2024-01-16T13:33:09.629535Z","shell.execute_reply":"2024-01-16T13:34:11.579668Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter your Nomic API Key:  ·············································\n"}]},{"cell_type":"code","source":"import pandas as pd \n\ndf = pd.read_excel('/kaggle/input/korean-hate-speech-sum/filtered_data.xlsx')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-16T13:34:45.233970Z","iopub.execute_input":"2024-01-16T13:34:45.234902Z","iopub.status.idle":"2024-01-16T13:34:56.833172Z","shell.execute_reply.started":"2024-01-16T13:34:45.234865Z","shell.execute_reply":"2024-01-16T13:34:56.832134Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                                text  class\n0  ....한국적인 미인의 대표적인 분...너무나 곱고아름다운모습...그모습뒤의 슬픔을...      0\n1                 1,2화 어설펐는데 3,4화 지나서부터는 갈수록 너무 재밌던데      0\n2                                  10+8 진짜 이승기랑 비교된다      0\n3                 10년뒤 윤서인은 분명히 재평가될것임. 말하나하나가 틀린게없음      0\n4                  10살 차이가 넘을텐데 부부라고? 무슨 내용인지 긍금하네..      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>....한국적인 미인의 대표적인 분...너무나 곱고아름다운모습...그모습뒤의 슬픔을...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1,2화 어설펐는데 3,4화 지나서부터는 갈수록 너무 재밌던데</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10+8 진짜 이승기랑 비교된다</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10년뒤 윤서인은 분명히 재평가될것임. 말하나하나가 틀린게없음</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10살 차이가 넘을텐데 부부라고? 무슨 내용인지 긍금하네..</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import re\nimport emoji\nfrom soynlp.normalizer import repeat_normalize\n\ndef text_preprocess(text):\n    pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-ㅣ가-힣]+')\n    url_pattern = re.compile(\n        r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n \n    text = pattern.sub(' ', text)\n    text = emoji.replace_emoji(text, replace='') #emoji 삭제\n    text = url_pattern.sub('', text)\n    text = text.strip()\n    text = repeat_normalize(text, num_repeats=2)\n    \n    return text\n\ndf['text'] = df['text'].apply(lambda x: text_preprocess(x))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-16T13:34:56.834403Z","iopub.execute_input":"2024-01-16T13:34:56.834724Z","iopub.status.idle":"2024-01-16T13:35:10.332710Z","shell.execute_reply.started":"2024-01-16T13:34:56.834697Z","shell.execute_reply":"2024-01-16T13:35:10.331813Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                                text  class\n0  ....한국적인 미인의 대표적인 분...너무나 곱고아름다운모습...그모습뒤의 슬픔을...      0\n1                 1,2화 어설펐는데 3,4화 지나서부터는 갈수록 너무 재밌던데      0\n2                                  10+8 진짜 이승기랑 비교된다      0\n3                 10년뒤 윤서인은 분명히 재평가될것임. 말하나하나가 틀린게없음      0\n4                  10살 차이가 넘을텐데 부부라고? 무슨 내용인지 긍금하네..      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>....한국적인 미인의 대표적인 분...너무나 곱고아름다운모습...그모습뒤의 슬픔을...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1,2화 어설펐는데 3,4화 지나서부터는 갈수록 너무 재밌던데</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10+8 진짜 이승기랑 비교된다</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10년뒤 윤서인은 분명히 재평가될것임. 말하나하나가 틀린게없음</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10살 차이가 넘을텐데 부부라고? 무슨 내용인지 긍금하네..</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_name = \"beomi/KcELECTRA-small-v2022\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=11, output_hidden_states=True)\n\n# set device\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T13:35:10.333904Z","iopub.execute_input":"2024-01-16T13:35:10.334178Z","iopub.status.idle":"2024-01-16T13:35:10.815037Z","shell.execute_reply.started":"2024-01-16T13:35:10.334155Z","shell.execute_reply":"2024-01-16T13:35:10.814154Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-small-v2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"ElectraForSequenceClassification(\n  (electra): ElectraModel(\n    (embeddings): ElectraEmbeddings(\n      (word_embeddings): Embedding(54343, 128, padding_idx=0)\n      (position_embeddings): Embedding(512, 128)\n      (token_type_embeddings): Embedding(2, 128)\n      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n    (encoder): ElectraEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=256, out_features=256, bias=True)\n              (key): Linear(in_features=256, out_features=256, bias=True)\n              (value): Linear(in_features=256, out_features=256, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=256, out_features=256, bias=True)\n              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=256, out_features=1024, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=1024, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): ElectraClassificationHead(\n    (dense): Linear(in_features=256, out_features=256, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=256, out_features=11, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"model.load_state_dict(torch.load('/kaggle/input/sentence-embedding-electra/sentence_embedding.pt'))\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-01-16T13:35:32.906708Z","iopub.execute_input":"2024-01-16T13:35:32.907424Z","iopub.status.idle":"2024-01-16T13:35:33.660963Z","shell.execute_reply.started":"2024-01-16T13:35:32.907390Z","shell.execute_reply":"2024-01-16T13:35:33.660037Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"ElectraForSequenceClassification(\n  (electra): ElectraModel(\n    (embeddings): ElectraEmbeddings(\n      (word_embeddings): Embedding(54343, 128, padding_idx=0)\n      (position_embeddings): Embedding(512, 128)\n      (token_type_embeddings): Embedding(2, 128)\n      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n    (encoder): ElectraEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=256, out_features=256, bias=True)\n              (key): Linear(in_features=256, out_features=256, bias=True)\n              (value): Linear(in_features=256, out_features=256, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=256, out_features=256, bias=True)\n              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=256, out_features=1024, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=1024, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): ElectraClassificationHead(\n    (dense): Linear(in_features=256, out_features=256, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=256, out_features=11, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\n\nbatch_size =32\n\n# Custom Dataset 클래스\nclass MyDataset(Dataset):\n    def __init__(self, dataframe):\n        self.dataframe = dataframe\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        text = self.dataframe.iloc[idx, 0]\n        label = self.dataframe.iloc[idx, 1]\n        return text, label\n\nmy_dataset = MyDataset(df)\ntrain_dataloader = DataLoader(my_dataset, shuffle=False, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T13:35:50.772890Z","iopub.execute_input":"2024-01-16T13:35:50.773329Z","iopub.status.idle":"2024-01-16T13:35:50.780544Z","shell.execute_reply.started":"2024-01-16T13:35:50.773297Z","shell.execute_reply":"2024-01-16T13:35:50.779488Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# 임베딩을 저장할 리스트 초기화\nembeddings = []\n\ntrain_dataloader = DataLoader(my_dataset, shuffle=False, batch_size=batch_size)\n# 데이터 로더를 통해 배치 단위로 처리\nfor texts, labgel in train_dataloader:\n    inputs = tokenizer(texts, padding=True, return_tensors='pt')\n    inputs = {k: v.to(device) for k,v in inputs.items()}\n    with torch.no_grad():\n        outputs = model(**inputs)\n        cls_hidden_state = outputs.hidden_states[-1].cpu()\n        mean_embedding = torch.mean(cls_hidden_state,dim=1).squeeze()\n        embeddings.extend(mean_embedding)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T13:35:58.354889Z","iopub.execute_input":"2024-01-16T13:35:58.355562Z","iopub.status.idle":"2024-01-16T13:37:29.993075Z","shell.execute_reply.started":"2024-01-16T13:35:58.355529Z","shell.execute_reply":"2024-01-16T13:37:29.992221Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# 일반적 \n# 0. 일반발언 common \n# 1. 성차별 sex\n# 2. 인종차별 \n# 3. 종교차별 religion\n# 4. 연령차별 age\n# 5. 정치차별 political\n# 6. 혐오욕설 \n# 7. 출신차별\n# 8. 외모차별\n# 9. 성소수자 차별\n# 10. 기타혐오 other \nclass_dictionary = {\n    0 : \"normal\" , 1: \"sex\", 2: \"race\", 3: \"religion\", 4:\"age\", \n    5: \"political\", 6: \"abuse\", 7 : \"origin\", 8: \"appearance\", \n    9: \"sexual minority\", 10:\"other\"\n}\n\ndf['embbeded_sentence'] = embeddings\ndf['class'] = df['class'].apply(lambda x : class_dictionary[x])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-16T13:40:05.506519Z","iopub.execute_input":"2024-01-16T13:40:05.507200Z","iopub.status.idle":"2024-01-16T13:40:09.486491Z","shell.execute_reply.started":"2024-01-16T13:40:05.507169Z","shell.execute_reply":"2024-01-16T13:40:09.485505Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                                                text   class  \\\n0  ....한국적인 미인의 대표적인 분...너무나 곱고아름다운모습...그모습뒤의 슬픔을...  normal   \n1                 1,2화 어설펐는데 3,4화 지나서부터는 갈수록 너무 재밌던데  normal   \n2                                  10+8 진짜 이승기랑 비교된다  normal   \n3                 10년뒤 윤서인은 분명히 재평가될것임. 말하나하나가 틀린게없음  normal   \n4                  10살 차이가 넘을텐데 부부라고? 무슨 내용인지 긍금하네..  normal   \n\n                                   embbeded_sentence  \n0  [tensor(0.9999), tensor(0.5262), tensor(-1.154...  \n1  [tensor(0.9422), tensor(0.5321), tensor(-1.196...  \n2  [tensor(0.8560), tensor(0.2837), tensor(-0.699...  \n3  [tensor(1.1205), tensor(0.2194), tensor(-0.887...  \n4  [tensor(-0.1273), tensor(0.2268), tensor(-0.37...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>class</th>\n      <th>embbeded_sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>....한국적인 미인의 대표적인 분...너무나 곱고아름다운모습...그모습뒤의 슬픔을...</td>\n      <td>normal</td>\n      <td>[tensor(0.9999), tensor(0.5262), tensor(-1.154...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1,2화 어설펐는데 3,4화 지나서부터는 갈수록 너무 재밌던데</td>\n      <td>normal</td>\n      <td>[tensor(0.9422), tensor(0.5321), tensor(-1.196...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10+8 진짜 이승기랑 비교된다</td>\n      <td>normal</td>\n      <td>[tensor(0.8560), tensor(0.2837), tensor(-0.699...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10년뒤 윤서인은 분명히 재평가될것임. 말하나하나가 틀린게없음</td>\n      <td>normal</td>\n      <td>[tensor(1.1205), tensor(0.2194), tensor(-0.887...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10살 차이가 넘을텐데 부부라고? 무슨 내용인지 긍금하네..</td>\n      <td>normal</td>\n      <td>[tensor(-0.1273), tensor(0.2268), tensor(-0.37...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nsize = len(df)\n\nmatrix = np.zeros((size, 256))\nclass_list = df['class']\n\nfor idx, vectors in enumerate(df['embbeded_sentence']):\n    vector = np.array(vectors)\n    matrix[idx] =vector ","metadata":{"execution":{"iopub.status.busy":"2024-01-16T13:42:01.308423Z","iopub.execute_input":"2024-01-16T13:42:01.308885Z","iopub.status.idle":"2024-01-16T13:42:02.683040Z","shell.execute_reply.started":"2024-01-16T13:42:01.308846Z","shell.execute_reply":"2024-01-16T13:42:02.682056Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\nvectors_tsne = tsne.fit_transform(matrix)\nvectors_tsne[:10]","metadata":{"execution":{"iopub.status.busy":"2024-01-16T13:44:02.414755Z","iopub.execute_input":"2024-01-16T13:44:02.415660Z","iopub.status.idle":"2024-01-16T13:53:11.381575Z","shell.execute_reply.started":"2024-01-16T13:44:02.415624Z","shell.execute_reply":"2024-01-16T13:53:11.380841Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"[t-SNE] Computing 121 nearest neighbors...\n[t-SNE] Indexed 153719 samples in 0.028s...\n[t-SNE] Computed neighbors for 153719 samples in 146.621s...\n[t-SNE] Computed conditional probabilities for sample 1000 / 153719\n[t-SNE] Computed conditional probabilities for sample 2000 / 153719\n[t-SNE] Computed conditional probabilities for sample 3000 / 153719\n[t-SNE] Computed conditional probabilities for sample 4000 / 153719\n[t-SNE] Computed conditional probabilities for sample 5000 / 153719\n[t-SNE] Computed conditional probabilities for sample 6000 / 153719\n[t-SNE] Computed conditional probabilities for sample 7000 / 153719\n[t-SNE] Computed conditional probabilities for sample 8000 / 153719\n[t-SNE] Computed conditional probabilities for sample 9000 / 153719\n[t-SNE] Computed conditional probabilities for sample 10000 / 153719\n[t-SNE] Computed conditional probabilities for sample 11000 / 153719\n[t-SNE] Computed conditional probabilities for sample 12000 / 153719\n[t-SNE] Computed conditional probabilities for sample 13000 / 153719\n[t-SNE] Computed conditional probabilities for sample 14000 / 153719\n[t-SNE] Computed conditional probabilities for sample 15000 / 153719\n[t-SNE] Computed conditional probabilities for sample 16000 / 153719\n[t-SNE] Computed conditional probabilities for sample 17000 / 153719\n[t-SNE] Computed conditional probabilities for sample 18000 / 153719\n[t-SNE] Computed conditional probabilities for sample 19000 / 153719\n[t-SNE] Computed conditional probabilities for sample 20000 / 153719\n[t-SNE] Computed conditional probabilities for sample 21000 / 153719\n[t-SNE] Computed conditional probabilities for sample 22000 / 153719\n[t-SNE] Computed conditional probabilities for sample 23000 / 153719\n[t-SNE] Computed conditional probabilities for sample 24000 / 153719\n[t-SNE] Computed conditional probabilities for sample 25000 / 153719\n[t-SNE] Computed conditional probabilities for sample 26000 / 153719\n[t-SNE] Computed conditional probabilities for sample 27000 / 153719\n[t-SNE] Computed conditional probabilities for sample 28000 / 153719\n[t-SNE] Computed conditional probabilities for sample 29000 / 153719\n[t-SNE] Computed conditional probabilities for sample 30000 / 153719\n[t-SNE] Computed conditional probabilities for sample 31000 / 153719\n[t-SNE] Computed conditional probabilities for sample 32000 / 153719\n[t-SNE] Computed conditional probabilities for sample 33000 / 153719\n[t-SNE] Computed conditional probabilities for sample 34000 / 153719\n[t-SNE] Computed conditional probabilities for sample 35000 / 153719\n[t-SNE] Computed conditional probabilities for sample 36000 / 153719\n[t-SNE] Computed conditional probabilities for sample 37000 / 153719\n[t-SNE] Computed conditional probabilities for sample 38000 / 153719\n[t-SNE] Computed conditional probabilities for sample 39000 / 153719\n[t-SNE] Computed conditional probabilities for sample 40000 / 153719\n[t-SNE] Computed conditional probabilities for sample 41000 / 153719\n[t-SNE] Computed conditional probabilities for sample 42000 / 153719\n[t-SNE] Computed conditional probabilities for sample 43000 / 153719\n[t-SNE] Computed conditional probabilities for sample 44000 / 153719\n[t-SNE] Computed conditional probabilities for sample 45000 / 153719\n[t-SNE] Computed conditional probabilities for sample 46000 / 153719\n[t-SNE] Computed conditional probabilities for sample 47000 / 153719\n[t-SNE] Computed conditional probabilities for sample 48000 / 153719\n[t-SNE] Computed conditional probabilities for sample 49000 / 153719\n[t-SNE] Computed conditional probabilities for sample 50000 / 153719\n[t-SNE] Computed conditional probabilities for sample 51000 / 153719\n[t-SNE] Computed conditional probabilities for sample 52000 / 153719\n[t-SNE] Computed conditional probabilities for sample 53000 / 153719\n[t-SNE] Computed conditional probabilities for sample 54000 / 153719\n[t-SNE] Computed conditional probabilities for sample 55000 / 153719\n[t-SNE] Computed conditional probabilities for sample 56000 / 153719\n[t-SNE] Computed conditional probabilities for sample 57000 / 153719\n[t-SNE] Computed conditional probabilities for sample 58000 / 153719\n[t-SNE] Computed conditional probabilities for sample 59000 / 153719\n[t-SNE] Computed conditional probabilities for sample 60000 / 153719\n[t-SNE] Computed conditional probabilities for sample 61000 / 153719\n[t-SNE] Computed conditional probabilities for sample 62000 / 153719\n[t-SNE] Computed conditional probabilities for sample 63000 / 153719\n[t-SNE] Computed conditional probabilities for sample 64000 / 153719\n[t-SNE] Computed conditional probabilities for sample 65000 / 153719\n[t-SNE] Computed conditional probabilities for sample 66000 / 153719\n[t-SNE] Computed conditional probabilities for sample 67000 / 153719\n[t-SNE] Computed conditional probabilities for sample 68000 / 153719\n[t-SNE] Computed conditional probabilities for sample 69000 / 153719\n[t-SNE] Computed conditional probabilities for sample 70000 / 153719\n[t-SNE] Computed conditional probabilities for sample 71000 / 153719\n[t-SNE] Computed conditional probabilities for sample 72000 / 153719\n[t-SNE] Computed conditional probabilities for sample 73000 / 153719\n[t-SNE] Computed conditional probabilities for sample 74000 / 153719\n[t-SNE] Computed conditional probabilities for sample 75000 / 153719\n[t-SNE] Computed conditional probabilities for sample 76000 / 153719\n[t-SNE] Computed conditional probabilities for sample 77000 / 153719\n[t-SNE] Computed conditional probabilities for sample 78000 / 153719\n[t-SNE] Computed conditional probabilities for sample 79000 / 153719\n[t-SNE] Computed conditional probabilities for sample 80000 / 153719\n[t-SNE] Computed conditional probabilities for sample 81000 / 153719\n[t-SNE] Computed conditional probabilities for sample 82000 / 153719\n[t-SNE] Computed conditional probabilities for sample 83000 / 153719\n[t-SNE] Computed conditional probabilities for sample 84000 / 153719\n[t-SNE] Computed conditional probabilities for sample 85000 / 153719\n[t-SNE] Computed conditional probabilities for sample 86000 / 153719\n[t-SNE] Computed conditional probabilities for sample 87000 / 153719\n[t-SNE] Computed conditional probabilities for sample 88000 / 153719\n[t-SNE] Computed conditional probabilities for sample 89000 / 153719\n[t-SNE] Computed conditional probabilities for sample 90000 / 153719\n[t-SNE] Computed conditional probabilities for sample 91000 / 153719\n[t-SNE] Computed conditional probabilities for sample 92000 / 153719\n[t-SNE] Computed conditional probabilities for sample 93000 / 153719\n[t-SNE] Computed conditional probabilities for sample 94000 / 153719\n[t-SNE] Computed conditional probabilities for sample 95000 / 153719\n[t-SNE] Computed conditional probabilities for sample 96000 / 153719\n[t-SNE] Computed conditional probabilities for sample 97000 / 153719\n[t-SNE] Computed conditional probabilities for sample 98000 / 153719\n[t-SNE] Computed conditional probabilities for sample 99000 / 153719\n[t-SNE] Computed conditional probabilities for sample 100000 / 153719\n[t-SNE] Computed conditional probabilities for sample 101000 / 153719\n[t-SNE] Computed conditional probabilities for sample 102000 / 153719\n[t-SNE] Computed conditional probabilities for sample 103000 / 153719\n[t-SNE] Computed conditional probabilities for sample 104000 / 153719\n[t-SNE] Computed conditional probabilities for sample 105000 / 153719\n[t-SNE] Computed conditional probabilities for sample 106000 / 153719\n[t-SNE] Computed conditional probabilities for sample 107000 / 153719\n[t-SNE] Computed conditional probabilities for sample 108000 / 153719\n[t-SNE] Computed conditional probabilities for sample 109000 / 153719\n[t-SNE] Computed conditional probabilities for sample 110000 / 153719\n[t-SNE] Computed conditional probabilities for sample 111000 / 153719\n[t-SNE] Computed conditional probabilities for sample 112000 / 153719\n[t-SNE] Computed conditional probabilities for sample 113000 / 153719\n[t-SNE] Computed conditional probabilities for sample 114000 / 153719\n[t-SNE] Computed conditional probabilities for sample 115000 / 153719\n[t-SNE] Computed conditional probabilities for sample 116000 / 153719\n[t-SNE] Computed conditional probabilities for sample 117000 / 153719\n[t-SNE] Computed conditional probabilities for sample 118000 / 153719\n[t-SNE] Computed conditional probabilities for sample 119000 / 153719\n[t-SNE] Computed conditional probabilities for sample 120000 / 153719\n[t-SNE] Computed conditional probabilities for sample 121000 / 153719\n[t-SNE] Computed conditional probabilities for sample 122000 / 153719\n[t-SNE] Computed conditional probabilities for sample 123000 / 153719\n[t-SNE] Computed conditional probabilities for sample 124000 / 153719\n[t-SNE] Computed conditional probabilities for sample 125000 / 153719\n[t-SNE] Computed conditional probabilities for sample 126000 / 153719\n[t-SNE] Computed conditional probabilities for sample 127000 / 153719\n[t-SNE] Computed conditional probabilities for sample 128000 / 153719\n[t-SNE] Computed conditional probabilities for sample 129000 / 153719\n[t-SNE] Computed conditional probabilities for sample 130000 / 153719\n[t-SNE] Computed conditional probabilities for sample 131000 / 153719\n[t-SNE] Computed conditional probabilities for sample 132000 / 153719\n[t-SNE] Computed conditional probabilities for sample 133000 / 153719\n[t-SNE] Computed conditional probabilities for sample 134000 / 153719\n[t-SNE] Computed conditional probabilities for sample 135000 / 153719\n[t-SNE] Computed conditional probabilities for sample 136000 / 153719\n[t-SNE] Computed conditional probabilities for sample 137000 / 153719\n[t-SNE] Computed conditional probabilities for sample 138000 / 153719\n[t-SNE] Computed conditional probabilities for sample 139000 / 153719\n[t-SNE] Computed conditional probabilities for sample 140000 / 153719\n[t-SNE] Computed conditional probabilities for sample 141000 / 153719\n[t-SNE] Computed conditional probabilities for sample 142000 / 153719\n[t-SNE] Computed conditional probabilities for sample 143000 / 153719\n[t-SNE] Computed conditional probabilities for sample 144000 / 153719\n[t-SNE] Computed conditional probabilities for sample 145000 / 153719\n[t-SNE] Computed conditional probabilities for sample 146000 / 153719\n[t-SNE] Computed conditional probabilities for sample 147000 / 153719\n[t-SNE] Computed conditional probabilities for sample 148000 / 153719\n[t-SNE] Computed conditional probabilities for sample 149000 / 153719\n[t-SNE] Computed conditional probabilities for sample 150000 / 153719\n[t-SNE] Computed conditional probabilities for sample 151000 / 153719\n[t-SNE] Computed conditional probabilities for sample 152000 / 153719\n[t-SNE] Computed conditional probabilities for sample 153000 / 153719\n[t-SNE] Computed conditional probabilities for sample 153719 / 153719\n[t-SNE] Mean sigma: 0.456518\n[t-SNE] KL divergence after 250 iterations with early exaggeration: 102.444824\n[t-SNE] KL divergence after 300 iterations: 4.799431\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"array([[ -8.102011  ,   4.329313  ],\n       [-11.280683  ,   0.94274604],\n       [ -2.9872715 ,  -2.1627765 ],\n       [ -6.169154  ,   1.434594  ],\n       [ -1.8016375 ,  -8.763573  ],\n       [-11.769028  ,  -0.5025457 ],\n       [ -1.5053109 ,  -8.937902  ],\n       [ -4.9061766 ,  -0.71799934],\n       [ -5.4378276 ,  -7.4274774 ],\n       [ -2.3602536 ,  -3.122212  ]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"dataset = []\n\nfor idx in range(len(df)):\n    text = df['text'][idx]\n    label = df['class'][idx]\n    dataset.append({\n        'text' : text, 'id': idx, 'label': label\n    })","metadata":{"execution":{"iopub.status.busy":"2024-01-16T13:53:46.036017Z","iopub.execute_input":"2024-01-16T13:53:46.036788Z","iopub.status.idle":"2024-01-16T13:53:48.993506Z","shell.execute_reply.started":"2024-01-16T13:53:46.036755Z","shell.execute_reply":"2024-01-16T13:53:48.992451Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from nomic import atlas\n\n# Create Atlas project\nproject = atlas.map_data(\n    data= dataset,\n    embeddings=vectors_tsne,\n    embedding_model=None,\n    projection=None,\n    identifier='label',\n    id_field='id'\n  )\n\nprint(project)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T14:04:41.686335Z","iopub.execute_input":"2024-01-16T14:04:41.686751Z","iopub.status.idle":"2024-01-16T14:04:48.124835Z","shell.execute_reply.started":"2024-01-16T14:04:41.686721Z","shell.execute_reply":"2024-01-16T14:04:48.123825Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"\u001b[32m2024-01-16 14:04:42.648\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m_create_project\u001b[0m:\u001b[36m842\u001b[0m - \u001b[1mCreating dataset `label`\u001b[0m\n\u001b[32m2024-01-16 14:04:42.761\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.atlas\u001b[0m:\u001b[36mmap_data\u001b[0m:\u001b[36m98\u001b[0m - \u001b[1mUploading data to Atlas.\u001b[0m\n\u001b[32m2024-01-16 14:04:42.945\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m_validate_and_correct_arrow_upload\u001b[0m:\u001b[36m338\u001b[0m - \u001b[33m\u001b[1mid_field is not a string. Converting to string from int32\u001b[0m\n31it [00:04,  7.23it/s]                        \n\u001b[32m2024-01-16 14:04:47.245\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m_add_data\u001b[0m:\u001b[36m1510\u001b[0m - \u001b[1mUpload succeeded.\u001b[0m\n\u001b[32m2024-01-16 14:04:47.248\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.atlas\u001b[0m:\u001b[36mmap_data\u001b[0m:\u001b[36m113\u001b[0m - \u001b[1m`tigere0723/label`: Data upload succeeded to dataset`\u001b[0m\n\u001b[32m2024-01-16 14:04:47.350\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36mcreate_index\u001b[0m:\u001b[36m1095\u001b[0m - \u001b[33m\u001b[1mYou did not specify the `topic_label_field` option in your topic_model, your dataset will not contain auto-labeled topics.\u001b[0m\n\u001b[32m2024-01-16 14:04:47.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36mcreate_index\u001b[0m:\u001b[36m1219\u001b[0m - \u001b[1mCreated map `label` in dataset `tigere0723/label`: https://atlas.nomic.ai/map/cee385e3-8b73-4a38-919f-c3ffd62b74af/e1d71557-8d05-493d-a3ef-fcdfb90e411e\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"label: https://atlas.nomic.ai/map/cee385e3-8b73-4a38-919f-c3ffd62b74af/e1d71557-8d05-493d-a3ef-fcdfb90e411e\n","output_type":"stream"}]}]}